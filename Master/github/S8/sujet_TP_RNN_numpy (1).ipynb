{"cells":[{"cell_type":"markdown","metadata":{"toc":true,"id":"R1dPRqZsdyKP"},"source":["<h1>Sommaire<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Construire-un-réseau-de-neurones-récurrents---étape-par-étape\" data-toc-modified-id=\"Construire-un-réseau-de-neurones-récurrents---étape-par-étape-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Construire un réseau de neurones récurrents - étape par étape</a></span><ul class=\"toc-item\"><li><span><a href=\"#Propagation-&quot;forward&quot;-à-travers-le-RNN-de-base\" data-toc-modified-id=\"Propagation-&quot;forward&quot;-à-travers-le-RNN-de-base-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Propagation \"forward\" à travers le RNN de base</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dimensions-des-entrées-$x$\" data-toc-modified-id=\"Dimensions-des-entrées-$x$-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Dimensions des entrées $x$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Entrée-avec-une-dimension-$n_x$\" data-toc-modified-id=\"Entrée-avec-une-dimension-$n_x$-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Entrée avec une dimension $n_x$</a></span></li><li><span><a href=\"#Nombre-de-pas-de-temps-pour-une-entrée-$x$-:-$T_{x}$\" data-toc-modified-id=\"Nombre-de-pas-de-temps-pour-une-entrée-$x$-:-$T_{x}$-1.1.1.2\"><span class=\"toc-item-num\">1.1.1.2&nbsp;&nbsp;</span>Nombre de pas de temps pour une entrée $x$ : $T_{x}$</a></span></li><li><span><a href=\"#Minibatches-de-taille-$m$\" data-toc-modified-id=\"Minibatches-de-taille-$m$-1.1.1.3\"><span class=\"toc-item-num\">1.1.1.3&nbsp;&nbsp;</span>Minibatches de taille $m$</a></span></li><li><span><a href=\"#Tenseur-3D-de-taille-$(n_{x},-m,-T_{x})$\" data-toc-modified-id=\"Tenseur-3D-de-taille-$(n_{x},-m,-T_{x})$-1.1.1.4\"><span class=\"toc-item-num\">1.1.1.4&nbsp;&nbsp;</span>Tenseur 3D de taille $(n_{x}, m, T_{x})$</a></span></li><li><span><a href=\"#Prendre-une-&quot;tranche&quot;-2D-pour-chaque-pas-de-temps-:-$x^{\\langle-t-\\rangle}$\" data-toc-modified-id=\"Prendre-une-&quot;tranche&quot;-2D-pour-chaque-pas-de-temps-:-$x^{\\langle-t-\\rangle}$-1.1.1.5\"><span class=\"toc-item-num\">1.1.1.5&nbsp;&nbsp;</span>Prendre une \"tranche\" 2D pour chaque pas de temps : $x^{\\langle t \\rangle}$</a></span></li></ul></li><li><span><a href=\"#Définition-de-l'état-caché-$a$\" data-toc-modified-id=\"Définition-de-l'état-caché-$a$-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Définition de l'état caché $a$</a></span></li><li><span><a href=\"#Dimensions-de-l'état-caché-$a$\" data-toc-modified-id=\"Dimensions-de-l'état-caché-$a$-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Dimensions de l'état caché $a$</a></span></li><li><span><a href=\"#Dimensions-de-la-prédiction-$\\hat{y}$\" data-toc-modified-id=\"Dimensions-de-la-prédiction-$\\hat{y}$-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Dimensions de la prédiction $\\hat{y}$</a></span></li></ul></li><li><span><a href=\"#Cellule-RNN-ou-&quot;RNN-cell&quot;\" data-toc-modified-id=\"Cellule-RNN-ou-&quot;RNN-cell&quot;-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Cellule RNN ou \"RNN cell\"</a></span><ul class=\"toc-item\"><li><span><a href=\"#rnn-cell-versus-rnn_cell_forward\" data-toc-modified-id=\"rnn-cell-versus-rnn_cell_forward-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>rnn cell versus rnn_cell_forward</a></span></li><li><span><a href=\"#Infos-additionelles\" data-toc-modified-id=\"Infos-additionelles-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Infos additionelles</a></span></li></ul></li><li><span><a href=\"#1.2---Passe-forward-du-RNN-simple\" data-toc-modified-id=\"1.2---Passe-forward-du-RNN-simple-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>1.2 - Passe forward du RNN simple</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Infos-additionnelles\" data-toc-modified-id=\"Infos-additionnelles-1.3.0.1\"><span class=\"toc-item-num\">1.3.0.1&nbsp;&nbsp;</span>Infos additionnelles</a></span></li><li><span><a href=\"#Situations-dans-lesquelles-ce-RNN-sera-plus-performant-:\" data-toc-modified-id=\"Situations-dans-lesquelles-ce-RNN-sera-plus-performant-:-1.3.0.2\"><span class=\"toc-item-num\">1.3.0.2&nbsp;&nbsp;</span>Situations dans lesquelles ce RNN sera plus performant :</a></span></li></ul></li></ul></li><li><span><a href=\"#Rétropropagation-dans-les-réseaux-neuronaux-récurrents\" data-toc-modified-id=\"Rétropropagation-dans-les-réseaux-neuronaux-récurrents-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Rétropropagation dans les réseaux neuronaux récurrents</a></span><ul class=\"toc-item\"><li><span><a href=\"#Passe-backward-du-RNN\" data-toc-modified-id=\"Passe-backward-du-RNN-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Passe backward du RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implémentation-de-rnn_cell_backward\" data-toc-modified-id=\"Implémentation-de-rnn_cell_backward-1.4.1.1\"><span class=\"toc-item-num\">1.4.1.1&nbsp;&nbsp;</span>Implémentation de rnn_cell_backward</a></span></li><li><span><a href=\"#Passe-backward-du-RNN\" data-toc-modified-id=\"Passe-backward-du-RNN-1.4.1.2\"><span class=\"toc-item-num\">1.4.1.2&nbsp;&nbsp;</span>Passe backward du RNN</a></span></li></ul></li></ul></li></ul></li></ul></div>"]},{"cell_type":"markdown","metadata":{"id":"AyYaroWfELFo"},"source":["# Construire un réseau de neurones récurrents - étape par étape\n","\n","Dans ce TP, vous allez mettre en œuvre les éléments clés d'un réseau neuronal récurrent en numpy.\n","\n","Les réseaux de neurones récurrents (RNN) sont très efficaces pour le traitement du langage naturel et d'autres tâches séquentielles car ils ont de la \"mémoire\". Ils lisent les entrées $x^{\\langle t \\rangle}$, comme des mots par exemple, un à la fois, et se souviennent de certaines informations / du contexte grâce aux activations de la couche cachée qui passent d'une étape à l'autre. Cela permet à un RNN unidirectionnel de prendre des informations du passé pour traiter les entrées ultérieures. Un RNN bidirectionnel peut prendre en comtpe le contexte à la fois du passé et du futur. \n","\n","\n","**Notation**:\n","- L'exposant (\"**super**script\") $[l]$ désigne un objet associé à la couche $l^{th}$. \n","\n","- L'exposant $(i)$ désigne un objet associé à l'exemple $i^{th}$. \n","\n","-  L'exposant $\\langle t \\rangle$ désigne un objet au pas de temps $t^{th}$. \n","    \n","- L'indice (\"**Sub**script\") $i$ indique l'entrée $i^{th}$ d'un vecteur.\n","\n","Exemple:  \n","- $a^{(2)[3]<4>}_5$ indique l'activation du 2ème exemple d'entraînement (2), de la 3ème couche [3], du 4ème pas de temps <4> et de la 5ème entrée dans le vecteur."]},{"cell_type":"markdown","metadata":{"id":"Lyejzf2TELFr"},"source":["Commençons par importer tous les modules nécessaires."]},{"cell_type":"code","source":["!wget --quiet https://www.irit.fr/~Thomas.Pellegrini/ens/RNN/RNN_in_numpy/rnn_utils.py"],"metadata":{"id":"cVyribI7g0jz","executionInfo":{"status":"ok","timestamp":1679043843361,"user_tz":-60,"elapsed":865,"user":{"displayName":"nour daoulatli","userId":"08598793982542183976"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2020-10-28T10:04:45.752009Z","start_time":"2020-10-28T10:04:43.826825Z"},"id":"fXWmy3GdELFr","executionInfo":{"status":"ok","timestamp":1679043847891,"user_tz":-60,"elapsed":4,"user":{"displayName":"nour daoulatli","userId":"08598793982542183976"}}},"outputs":[],"source":["import numpy as np\n","from rnn_utils import *"]},{"cell_type":"markdown","metadata":{"id":"03kGDM_sELFv"},"source":["## Propagation \"forward\" à travers le RNN de base\n","\n","Le RNN de base que vous mettrez en œuvre a la structure ci-dessous. Dans cet exemple, les séquences d'entrée et de sortie ont même durée : $T_x = T_y$. "]},{"cell_type":"markdown","metadata":{"id":"ZBxzkRzsELFv"},"source":["<img src=\"https://www.irit.fr/~Thomas.Pellegrini/ens/RNN/RNN_in_numpy/images/RNN.png\" style=\"width:500;height:300px;\">\n","<caption><center> **Figure 1**: modèle RNN simple </center></caption>"]},{"cell_type":"markdown","metadata":{"id":"jFNRtAP_ELFw"},"source":["### Dimensions des entrées $x$\n","\n","#### Entrée avec une dimension $n_x$\n","\n","* Pour un pas de temps unique d'un exemple d'entrée unique, $x^{(i) \\langle t \\rangle }$ est un vecteur.\n","\n","* En prenant le langage comme exemple, un langage avec un vocabulaire de 5000 mots pourrait être codé en une seule fois dans un vecteur qui a 5000 unités.  Ainsi, $x^{(i)\\langle t \\rangle}$ aurait la taille (5000,).  \n","\n","* Nous utiliserons la notation $n_x$ pour indiquer la dimension d'un seul exemple d'entraînement."]},{"cell_type":"markdown","metadata":{"id":"EnYGy4L-ELFx"},"source":["#### Nombre de pas de temps pour une entrée $x$ : $T_{x}$\n","* Un RNN a de multiples pas de temps, que nous indexerons avec $t$.\n","* Un exemple d'apprentissage $x^{(i)}$ est composé de plusieurs pas de temps $T_x$.  Par exemple, s'il y a 10 pas de temps, $T_{x} = 10$"]},{"cell_type":"markdown","metadata":{"id":"Azzhk7jCELFx"},"source":["#### Minibatches de taille $m$\n","* Soient des mini-batchs contenant chacun 20 exemples.  \n","* Pour bénéficier de la vectorisation, nous \"empilons\" 20 colonnes d'exemples $x^{(i)}$.\n","* Par une dimension de 5000 et 10 pas de temps pour les $x$, on aurait donc un tenseur de taille (5000, 20, 10). \n","* Nous utiliserons $m$ pour indiquer le nombre d'exemples des mini-batches.  \n","* La taille d'un mini-batch dans cet exercice est : $(n_x, m, T_x)$"]},{"cell_type":"markdown","metadata":{"id":"qNR7VozOELFy"},"source":["#### Tenseur 3D de taille $(n_{x}, m, T_{x})$\n","* Le tenseur 3D $x$ de taille $(n_x, m, T_x)$ représente l'entrée $x$ qui est donnée au RNN.\n","\n","#### Prendre une \"tranche\" 2D pour chaque pas de temps : $x^{\\langle t \\rangle}$\n","* A chaque étape, nous utiliserons un mini-batch d'exemples.\n","* Donc, pour chaque pas de temps $t$, nous utiliserons une tranche  2D de dimension $(n_x, m)$.\n","* Nous appelons cette tranche 2D $x^{\\langle t \\rangle}$.  Le nom de la variable dans le code est `xt`."]},{"cell_type":"markdown","metadata":{"id":"VOzhhTj4ELFy"},"source":["### Définition de l'état caché $a$\n","\n","* L'activation $a^{\\langle t \\rangle}$ qui est passée au RNN d'un pas de temps à l'autre est appelée \"état caché\".\n","\n","### Dimensions de l'état caché $a$\n","\n","* Comme le tenseur d'entrée $x$, l'état caché pour un exemple unique est un vecteur de longueur $n_{a}$.\n","* Si nous incluons un mini-batch d'exemples $m$, la forme d'un mini-batch est $(n_{a}, m)$.\n","* Si nous incluons la dimension du pas de temps, la dimension de l'état caché est $(n_{a}, m, T_x)$.\n","* Nous allons faire une boucle à travers les pas de temps avec l'index $t$, et travailler avec une tranche 2D du tenseur 3D.  \n","* Nous appellerons cette tranche 2D $a^{\\langle t \\rangle}$. \n","* Dans le code, les noms de variables que nous utilisons sont soit `a_prev` ou `a_next`, selon la fonction qui est implémentée.\n","* La dimension de cette tranche 2D est $(n_{a}, m)$"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-10-28T10:23:24.751284Z","start_time":"2020-10-28T10:23:24.738271Z"},"id":"fvX6MK8hdyKd"},"source":["### Dimensions de la prédiction $\\hat{y}$\n","* Similaire aux entrées et aux états cachés, $\\hat{y}$ est un tenseur 3D de la forme $(n_{y}, m, T_{y})$.\n","    * $n_{y}$ : dimension du vecteur représentant la prédiction.\n","    * $m$ : nombre d'exemples dans un mini-batch.\n","    * $T_{y}$ : nombre de pas de temps de la prédiction.\n","* Pour un seul pas de temps $t$, une tranche 2D $\\hat{y}^{\\langle t \\rangle}$ a la dimension $(n_{y}, m)$.\n","* Dans le code, les noms des variables sont\n","    - `y_pred` : $\\hat{y}$ \n","    - `yt_pred` : $\\hat{y}^{\\langle t \\rangle}$"]},{"cell_type":"markdown","metadata":{"id":"9ZrlQ4X8ELFz"},"source":["Voici comment implémenter un RNN : \n","\n","**Étapes** :\n","1. Implémenter les calculs nécessaires pour un pas de temps du RNN.\n","2. Implémenter une boucle sur les $T_x$ pas de temps afin de traiter toutes les entrées, une à la fois. "]},{"cell_type":"markdown","metadata":{"id":"6oXWAKeTELF0"},"source":["## Cellule RNN ou \"RNN cell\"\n","\n","Un RNN peut être considéré comme l'utilisation répétée d'une seule cellule ou cell. Il faut d'abord effectuer les calculs pour un seul pas de temps. La figure suivante décrit les opérations pour un pas de temps d'une cellule RNN. \n","\n","<img src=\"https://www.irit.fr/~Thomas.Pellegrini/ens/RNN/RNN_in_numpy/images/rnn_step_forward_figure2_v3a.png\" style=\"width:700px;height:300px;\">\n","<caption><center> **Figure 2**: RNN cell simple. Prend comme entrée $x^{\\langle t \\rangle}$ (entrée au temps $t$) et $a^{\\langle t - 1\\rangle}$ (état caché au temps $t-1$) et génère $a^{\\langle t \\rangle}$, qui est ensuite donné  à la prochaine celulle RNN cell. $a^{\\langle t \\rangle}$ est aussi utilisé pour faire la prédiction $\\hat{y}^{\\langle t \\rangle}$ </center></caption>\n","\n","### rnn cell versus rnn_cell_forward\n","* Une RNN cell génère l'état caché $a^{\\langle t \\rangle}$.  \n","    * La RNN cell est montrée sur la figure à l'aide d'une ligne noire pleine.  \n","* La fonction à implémenter, `rnn_cell_forward`, calcule aussi la prédiction $\\hat{y}^{\\langle t \\rangle}$\n","    * rnn_cell_forward est montrée sur la figure à l'aide d'une ligne noire en pointillés. "]},{"cell_type":"markdown","metadata":{"id":"HhFLBKwbELF0"},"source":["**Exercise**: Implémenter RNN-cell telle que décrite dans la Figure (2).\n","\n","**Instructions**:\n","1. Calculer l'état caché avec l'activation tanh : $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n","2. En utilisant le nouvel état caché $a^{\\langle t \\rangle}$, calculer la prédiction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. La fonction `softmax` est donnée.\n","3. Sauvegarder $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ dans `cache`.\n","4. Returner $a^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$ et `cache`\n","\n","### Infos additionelles \n","* [numpy.tanh](https://www.google.com/search?q=numpy+tanh&rlz=1C5CHFA_enUS854US855&oq=numpy+tanh&aqs=chrome..69i57j0l5.1340j0j7&sourceid=chrome&ie=UTF-8)\n","* La fonction `softmax` est définie dans 'rnn_utils.py' et a déjà été importée ci-dessus.\n","* Pour les multiplications de matrices, utiliser [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2020-10-28T12:34:31.831320Z","start_time":"2020-10-28T12:34:31.822946Z"},"id":"fxI-F0HWELF1","executionInfo":{"status":"ok","timestamp":1679046236052,"user_tz":-60,"elapsed":286,"user":{"displayName":"nour daoulatli","userId":"08598793982542183976"}}},"outputs":[],"source":["def rnn_cell_forward(xt, a_prev, parameters):\n","    \"\"\"\n","    Réalise une unique étape forward de RNN-cell telle que décrit dans la figure (2)\n","\n","    Arguments:\n","    xt -- l'entrée au temps \"t\", array numpy de shape (n_x, m).\n","    a_prev -- État caché au temps \"t-1\", array numpy array of shape (n_a, m)\n","    parameters -- dictionnaire python contenant:\n","                        Wax -- matrice de poids multipliant l'entrée, array numpy de shape (n_a, n_x)\n","                        Waa -- matrice de poids multipliant l'état caché, array numpy de shape (n_a, n_a)\n","                        Wya -- matrice de poids reliant l'état caché à la sortie, array numpy de shape (n_y, n_a)\n","                        ba --  Bias, array numpy de shape (n_a, 1)\n","                        by -- Bias reliant reliant l'état caché à la sortie, array numpy de shape (n_y, 1)\n","    Retourne :\n","    a_next -- le prochain état caché, de shape (n_a, m)\n","    yt_pred -- prédiction au temps \"t\", array numpy de shape (n_y, m)\n","    cache -- tuple des valeurs utiles pour la passe backward, contient (a_next, a_prev, xt, parameters)\n","    \"\"\"\n","    \n","    # Récupérer les paramètres de \"parameters\"\n","    Wax = parameters[\"Wax\"]\n","    Waa = parameters[\"Waa\"]\n","    Wya = parameters[\"Wya\"]\n","    ba = parameters[\"ba\"]\n","    by = parameters[\"by\"]\n","    \n","    ### A COMPLETER ### \n","    # calcul du prochain état caché activation à l'aide de la formule donnée ci-dessus\n","    a_next = np.tanh(np.dot(Waa,a_prev) + np.dot(Wax,xt) + ba)\n","    # calcul de la prédiction au temps \"t\" à l'aide de la formule donnée ci-dessus\n","    yt_pred = softmax(np.dot(Wya,a_next) + by)\n","    \n","    # sauvegarde du cache pour la passe backward\n","    cache = (a_next, a_prev, xt, parameters)\n","    \n","    return a_next, yt_pred, cache"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"V03ZGazVELF4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679046240883,"user_tz":-60,"elapsed":701,"user":{"displayName":"nour daoulatli","userId":"08598793982542183976"}},"outputId":"3dd8720a-4015-4e2d-be5d-902cd21afcb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["a_next[4] = \n"," [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n"," -0.18887155  0.99815551  0.6531151   0.82872037]\n","a_next.shape = \n"," (5, 10)\n","yt_pred[1] =\n"," [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n"," 0.36920224 0.9966312  0.9982559  0.17746526]\n","yt_pred.shape = \n"," (2, 10)\n"]}],"source":["np.random.seed(1)\n","xt_tmp = np.random.randn(3,10)\n","a_prev_tmp = np.random.randn(5,10)\n","parameters_tmp = {}\n","parameters_tmp['Waa'] = np.random.randn(5,5)\n","parameters_tmp['Wax'] = np.random.randn(5,3)\n","parameters_tmp['Wya'] = np.random.randn(2,5)\n","parameters_tmp['ba'] = np.random.randn(5,1)\n","parameters_tmp['by'] = np.random.randn(2,1)\n","\n","a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n","print(\"a_next[4] = \\n\", a_next_tmp[4])\n","print(\"a_next.shape = \\n\", a_next_tmp.shape)\n","print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n","print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)"]},{"cell_type":"markdown","metadata":{"id":"c_6KJp0lELF7"},"source":["**Affichage attendu**: \n","```Python\n","a_next[4] = \n"," [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n"," -0.18887155  0.99815551  0.6531151   0.82872037]\n","a_next.shape = \n"," (5, 10)\n","yt_pred[1] =\n"," [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n","  0.36920224  0.9966312   0.9982559   0.17746526]\n","yt_pred.shape = \n"," (2, 10)\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"zNEf0L5udyKi"},"source":["## 1.2 - Passe forward du RNN simple \n","\n","- Un RNN est une répétition de la cellule RNN que vous venez de construire. \n","    - Si votre séquence de données d'entrée est longue de 10 pas de temps, alors vous réutiliserez la cellule RNN 10 fois. \n","- Chaque cellule prend deux entrées à chaque pas de temps :\n","    - $a^{\\langle t-1 \\rangle}$ : l'état caché de la cellule précédente.\n","    - $x^{\\langle t \\rangle}$ : les données d'entrée du pas de temps en cours.\n","- Il y a deux sorties à chaque pas de temps :\n","    - Un état caché ($a^{\\langle t \\rangle}$)\n","    - Une prédiction ($y^{\\langle t \\rangle}$)\n","- Les poids et biais $(W_{aa}, b_{a}, W_{ax}, b_{x})$ sont réutilisés à chaque pas de temps. \n","    - Ils sont conservés entre les appels à rnn_cell_forward dans le dictionnaire 'parameters'.\n","\n","\n","<img src=\"https://www.irit.fr/~Thomas.Pellegrini/ens/RNN/RNN_in_numpy/images/rnn_forward_sequence_figure3_v3a.png\" style=\"width:800px;height:180px;\">\n","<caption><center> **Figure 3**: RNN simple. La séquence d'entrée $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ a $T_x$ pas de temps. Le réseau génère $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n"]},{"cell_type":"markdown","metadata":{"id":"r104TTL-ELF8"},"source":["**Exercice**: Implémenter la passe forward du RNN décrit dans la Figure (3).\n","\n","**Instructions**:\n","* Créer un array 3D de zéros, $a$ de shape $(n_{a}, m, T_{x})$, qui va contenir tous les états cachés calculés par le RNN.\n","* Créer un array 3D de zéros, $\\hat{y}$, de shape $(n_{y}, m, T_{x})$ pour les prédictions.  \n","    - Dans notre exercice, $T_{y} = T_{x}$, ce qui est une simplification du cas général.\n","* Initialiser l'état caché 2D `a_next` en le mettant égal à l'átat caché initial $a_{0}$.\n","* Á chaque pas de temps $t$:\n","    - Récupérer $x^{\\langle t \\rangle}$, qui est la tranche 2D de $x$ au pas de temps $t$.\n","        - $x^{\\langle t \\rangle}$ a une shape $(n_{x}, m)$\n","        - $x$ a une shape $(n_{x}, m, T_{x})$\n","    - Actualiser l'état caché 2D $a^{\\langle t \\rangle}$ (variable `a_next`), la prédiction $\\hat{y}^{\\langle t \\rangle}$ et le cache en appelant `rnn_cell_forward`.\n","        - $a^{\\langle t \\rangle}$ a une shape $(n_{a}, m)$\n","    - Stocker l'état caché 2D dans le tenseur 3D $a$, à la position $t^{th}$.\n","        - $a$ a une shape $(n_{a}, m, T_{x})$\n","    - Stocker la prédiction 2D $\\hat{y}^{\\langle t \\rangle}$ (variable `yt_pred`) dans le tenseur 3D $\\hat{y}_{pred}$ au temps $t^{th}$.\n","        - $\\hat{y}^{\\langle t \\rangle}$ a une shape $(n_{y}, m)$\n","        - $\\hat{y}$ a une shape $(n_{y}, m, T_x)$\n","    - Faire un append du cache à la liste caches.\n","* Retourner le tenseur 3D $a$ et $\\hat{y}$, ainsi que caches.\n","\n","#### Infos additionnelles\n","- [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n","- Si vous manipulez un array numpy 3D et souhaitez récupérer une tranche en fonction d'un indice $i$ de la dimension 3, vous pouvez faire : `var_name[:,:,i]`."]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":true,"id":"VmeprGJpELF9","executionInfo":{"status":"ok","timestamp":1679047605324,"user_tz":-60,"elapsed":305,"user":{"displayName":"nour daoulatli","userId":"08598793982542183976"}}},"outputs":[],"source":["def rnn_forward(x, a0, parameters):\n","    \"\"\"\n","    Réalise une passe forward du RNN décrite dans la Figure (3).\n","\n","    Arguments :\n","    x -- mini-batch avec les séquences d'entrée, de shape (n_x, m, T_x).\n","    a0 -- État caché initial, de shape (n_a, m)\n","    parameters -- dictionnaire python contenant:\n","                        Wax -- matrice de poids multipliant l'entrée, array numpy de shape (n_a, n_x)\n","                        Waa -- matrice de poids multipliant l'état caché, array numpy de shape (n_a, n_a)\n","                        Wya -- matrice de poids reliant l'état caché à la sortie, array numpy de shape (n_y, n_a)\n","                        ba --  Bias, array numpy de shape (n_a, 1)\n","                        by -- Bias reliant reliant l'état caché à la sortie, array numpy de shape (n_y, 1)\n","    Retourne:\n","    a -- États cachés, de shape (n_a, m, T_x)\n","    y_pred -- Prédictions, de shape (n_y, m, T_x)\n","    caches -- tuple des valeurs nécessaires à la passe backward, contient (liste des caches, x)\n","    \"\"\"\n","    \n","    # Initialiser \"caches\" qui contiendra la liste des caches\n","    caches = []\n","    \n","    # Récupérer les dimensions de x et parameters[\"Wya\"]\n","    n_x, m, T_x = x.shape\n","    n_y, n_a = parameters[\"Wya\"].shape\n","    \n","    ### A COMPLETER ### \n","    \n","    # initialiser \"a\" et \"y_pred\" avec des zéros\n","    a = np.zeros((n_a,m,T_x))\n","    y_pred = np.zeros((n_y,m,T_x))\n","    \n","    # Initialiser a_next \n","    a_next = a0\n","    \n","    # loop sur les pas de temps de l'entrée 'x' \n","    for t in range(T_x):\n","        # Actualiser a_next, calculer yt_pred, et cache \n","        xt = x[:,:,t]\n","        a_next, yt_pred, cache = rnn_cell_forward(xt, a_next, parameters)\n","        # Stocker a_next\n","        a[:,:,t] = a_next\n","        # Stocker yt_pred\n","        y_pred[:,:,t] = yt_pred\n","        # Append \"cache\" à \"caches\" \n","        caches.append(cache)\n","        \n","    ### FIN ###\n","    \n","    # Stocker les variables nécessaires à la passe backward\n","    caches = (caches, x)\n","    \n","    return a, y_pred, caches"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":true,"id":"jEPrd77rELF_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679047609086,"user_tz":-60,"elapsed":291,"user":{"displayName":"nour daoulatli","userId":"08598793982542183976"}},"outputId":"554c8b56-12f8-43cc-a061-0668d0925b77"},"outputs":[{"output_type":"stream","name":"stdout","text":["a[4][1] = \n"," [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n","a.shape = \n"," (5, 10, 4)\n","y_pred[1][3] =\n"," [0.79560373 0.86224861 0.11118257 0.81515947]\n","y_pred.shape = \n"," (2, 10, 4)\n","caches[1][1][3] =\n"," [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n","len(caches) = \n"," 2\n"]}],"source":["np.random.seed(1)\n","x_tmp = np.random.randn(3,10,4)\n","a0_tmp = np.random.randn(5,10)\n","parameters_tmp = {}\n","parameters_tmp['Waa'] = np.random.randn(5,5)\n","parameters_tmp['Wax'] = np.random.randn(5,3)\n","parameters_tmp['Wya'] = np.random.randn(2,5)\n","parameters_tmp['ba'] = np.random.randn(5,1)\n","parameters_tmp['by'] = np.random.randn(2,1)\n","\n","a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n","print(\"a[4][1] = \\n\", a_tmp[4][1])\n","print(\"a.shape = \\n\", a_tmp.shape)\n","print(\"y_pred[1][3] =\\n\", y_pred_tmp[1][3])\n","print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n","print(\"caches[1][1][3] =\\n\", caches_tmp[1][1][3])\n","print(\"len(caches) = \\n\", len(caches_tmp))"]},{"cell_type":"markdown","metadata":{"id":"R135qjynELGC"},"source":["**Expected Output**:\n","\n","```Python\n","a[4][1] = \n"," [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n","a.shape = \n"," (5, 10, 4)\n","y_pred[1][3] =\n"," [ 0.79560373  0.86224861  0.11118257  0.81515947]\n","y_pred.shape = \n"," (2, 10, 4)\n","caches[1][1][3] =\n"," [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n","len(caches) = \n"," 2\n","```"]},{"cell_type":"markdown","metadata":{"id":"GANptfNiELGC"},"source":["L'implémentation de la passe forward est terminée.\n","\n","#### Situations dans lesquelles ce RNN sera plus performant :\n","- Il fonctionnera suffisamment bien pour certaines applications, mais il souffre des problèmes de dissipation du gradient (vanishing gradient). \n","- Le RNN fonctionne mieux lorsque chaque sortie $\\hat{y}^{\\langle t \\rangle}$ peut être estimée en utilisant le contexte \"local\".  \n","- Le contexte \"local\" fait référence aux informations qui sont proches du pas de temps de la prédiction $t$.\n","- Plus formellement, le contexte local se réfère aux entrées $x^{\\langle t' \\rangle}$ et aux prédictions $\\hat{y}^{\\langle t \\rangle}$ où $t'$ est proche de $t$."]},{"cell_type":"markdown","metadata":{"id":"hRg4ba60ELGW"},"source":["## Rétropropagation dans les réseaux neuronaux récurrents\n","\n","Dans les librairies d'apprentissage profond actuelles, il suffit d'implémenter la passe forward, et le framework s'occupe de la passe backward automatiquement, avec les calculs des gradients et des actualisations des poids faits automatique. Il est cependant intéressant de comprendre comment fonctionne la passe backward, en l'implémentant pour un RNN simple.\n","\n","Il faut calculer les dérivées par rapport à la fonction de perte pour ensuite mettre à jour les paramètres du RNN. Les équations de rétropropagation sont données ci-dessous. "]},{"cell_type":"markdown","metadata":{"id":"7TnuUyMPELGX"},"source":["Notez que ce notebook n'implémente pas la passe backward de la perte \"J\" vers \"a\". Cela aurait nécessité d'implémenter la couche dense avec softmax qui font partie de la passe forward. On suppose que ceci est calculé ailleurs et que le résultat est passé à rnn_backward dans une variable \"da\". On suppose en outre que la perte a été ajustée en fonction de la taille du minibatch (m) et que la division par le nombre d'exemples n'est pas nécessaire ici."]},{"cell_type":"markdown","metadata":{"id":"eN4Hc8P4ELGX"},"source":["### Passe backward du RNN\n","\n","Nous commencerons par calculer la passe backward pour la cellule RNN de base, puis, dans les sections suivantes, nous itérerons à travers les cellules.\n","\n","<img src=\"https://www.irit.fr/~Thomas.Pellegrini/ens/RNN/RNN_in_numpy/images/rnn_backward_overview_3a_1.png\" style=\"width:500;height:300px;\"> <br>\n","<caption><center> **Figure 4**: Passe backward pour une cell. Tout comme dans les réseaux entièrement connectés, la dérivée de la fonction de coût $J$ se propage dans les étapes temporelles du RNN en suivant la règle des dérivées en chaîne. À l'intérieur de la cellule, la règle est également utilisée pour calculer $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ pour actualiser les paramètres $(W_{ax}, W_{aa}, b_a)$. L'opération utilise le cache de la passe forward. </center></caption>"]},{"cell_type":"markdown","metadata":{"id":"nZb7R-V8ELGY"},"source":["L'abrégé de la dérivée partielle de la fonction de perte par rapport à une variable est souvent notée dVariable. Par exemple, $\\frac{\\partial J}{\\partial W_{ax}}$ est $dW_{ax}$. Nous utiliserons cette notation."]},{"cell_type":"markdown","metadata":{"id":"cyh9i4bFELGZ"},"source":["\n","<img src=\"https://www.irit.fr/~Thomas.Pellegrini/ens/RNN/RNN_in_numpy/images/rnn_cell_backward_3a_c.png\" style=\"width:800;height:500px;\"> <br>\n","<caption><center> **Figure 5** : cette implémentation de rnn_cell_backward n'inclut **pas** la couche dense de sortie et le softmax qui sont inclus dans rnn_cell_forward.  \n","\n","$da_{next}$ est $\\frac{\\partial{J}}{\\partial a^{\\langle t \\rangle}}$ et inclut la perte des étapes précédentes et de la sortie courante. Ce sont les flêches vertes que vous allez implémenter dans rnn_backward.  </center></caption>"]},{"cell_type":"markdown","metadata":{"id":"PeVJfrjkELGa"},"source":["##### Équations\n","Pour implémenter rnn_cell_backward, vous pouvez utiliser les équations suivantes. C'est un bon exercice que de les calculer à la main. Ici, $*$ indique une multiplication élément ar élément tandis que l'absence de symbole indique une multiplication matricielle.\n","\n","\\begin{align}\n","\\displaystyle a^{\\langle t \\rangle} &= \\tanh(W_{ax} x^{\\langle t \\rangle} + W_{aa} a^{\\langle t-1 \\rangle} + b_{a})\\tag{-} \\\\[8pt]\n","\\displaystyle \\frac{\\partial \\tanh(x)} {\\partial x} &= 1 - \\tanh^2(x) \\tag{-} \\\\[8pt]\n","\\displaystyle  {dW_{ax}} &= (da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) )) x^{\\langle t \\rangle T}\\tag{1} \\\\[8pt]\n","\\displaystyle dW_{aa} &= (da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))  a^{\\langle t-1 \\rangle T}\\tag{2} \\\\[8pt]\n","\\displaystyle db_a& = \\sum_{batch}( da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))\\tag{3} \\\\[8pt]\n","\\displaystyle dx^{\\langle t \\rangle} &= { W_{ax}}^T (da_{next} * ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))\\tag{4} \\\\[8pt]\n","\\displaystyle da_{prev} &= { W_{aa}}^T(da_{next} *  ( 1-\\tanh^2(W_{ax}x^{\\langle t \\rangle}+W_{aa} a^{\\langle t-1 \\rangle} + b_{a}) ))\\tag{5}\n","\\end{align}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FYskaCprELGa"},"source":["#### Implémentation de rnn_cell_backward\n","Les résultats peuvent être calculés directement en appliquant les équations ci-dessus. Toutefois, il est possible de simplifier les résultats en calculant \"dz\" et en appliquant la règle de la chaîne.  \n","Il est possible de simplifier encore plus les choses en notant que $\\tanh(W_{ax}x^{\\angle t \\rangle}+W_{aa} a^{\\angle t-1 \\rangle} + b_{a})$ a été calculé et stocké dans la passe forward. \n","\n","Pour calculer dba, le \"batch\" ci-dessus est la somme de tous les \"m\" exemples (axe = 1). Notez que vous devez utiliser l'option keepdims = True.\n","\n","Note : rnn_cell_backward n'inclut pas le calcul de la perte de $y \\langle t \\rangle$, ceci est incorporé dans le da_next entrant. Il s'agit d'un léger décalage avec rnn_cell_forward qui comprend une couche dense et un softmax. \n","\n","Note : dans le code:  \n","$\\displaystyle dx^{\\langle t \\rangle}$ est noté dxt,   \n","$\\displaystyle d W_{ax}$ est noté dWax,   \n","$\\displaystyle da_{prev}$ est noté da_prev,    \n","$\\displaystyle dW_{aa}$ est noté dWaa,   \n","$\\displaystyle db_{a}$ est noté dba,   \n","dz n'est pas dérivé ci-dessus mais peut être facultativement dérivé pour simplifier les calculs répétés.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"BmbWomYOELGb"},"outputs":[],"source":["def rnn_cell_backward(da_next, cache):\n","    \"\"\"\n","    Implémente la passe backward de RNN-cell (un seul pas de temps).\n","\n","    Arguments :\n","    da_next -- Gradient de la loss par rapport au prochaine état caché (a_next)\n","    cache -- dictionnaire python contenant des variables utiles (issu de rnn_cell_forward())\n","\n","    Retourne :\n","    gradients -- dictionnaire python contenant les gradients suivants :\n","                        dx -- Gradients par rapport aux input, de shape (n_x, m)\n","                        da_prev -- Gradients par rapport à l'état caché précédent (a_prev), de shape (n_a, m)\n","                        dWax -- Gradients par rapport aux poids input-to-hidden, de shape (n_a, n_x)\n","                        dWaa -- Gradients par rapport aux poids hidden-to-hidden, de shape (n_a, n_a)\n","                        dba -- Gradients par rapport au vecteur biais, de shape (n_a, 1)\n","    \"\"\"\n","    \n","    # Récupérer le cache\n","    (a_next, a_prev, xt, parameters) = cache\n","    \n","    # Récupérer les valeurs des paramètres :\n","    Wax = parameters[\"Wax\"]\n","    Waa = parameters[\"Waa\"]\n","    Wya = parameters[\"Wya\"]\n","    ba = parameters[\"ba\"]\n","    by = parameters[\"by\"]\n","\n","    ### A COMPLETER ### \n","    # calculer le gradient de la perte par rapport à z (optionel) \n","    dz = None\n","\n","    # calculer le gradient de la perte par rapport à Wax\n","    dxt = None\n","    dWax = None\n","\n","    # compute the gradient with respect to Waa (≈2 lines)\n","    da_prev = None\n","    dWaa = None\n","\n","    # calculer le gradient de la perte par rapport à b \n","    dba = None\n","\n","    ### FIN ###\n","    \n","    # Stocker les gradients dans un dictionnaire\n","    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n","    \n","    return gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"VVbG5aPtELGd","scrolled":true},"outputs":[],"source":["np.random.seed(1)\n","xt_tmp = np.random.randn(3,10)\n","a_prev_tmp = np.random.randn(5,10)\n","parameters_tmp = {}\n","parameters_tmp['Wax'] = np.random.randn(5,3)\n","parameters_tmp['Waa'] = np.random.randn(5,5)\n","parameters_tmp['Wya'] = np.random.randn(2,5)\n","parameters_tmp['ba'] = np.random.randn(5,1)\n","parameters_tmp['by'] = np.random.randn(2,1)\n","\n","a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n","\n","da_next_tmp = np.random.randn(5,10)\n","gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)\n","print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n","print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n","print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n","print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n","print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n","print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n","print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n","print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n","print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"]},{"cell_type":"markdown","metadata":{"id":"8rDmdFwaELGf"},"source":["**Affichage attendu** :\n","\n","<table>\n","    <tr>\n","        <td>\n","            **gradients[\"dxt\"][1][2]** =\n","        </td>\n","        <td>\n","           -1.3872130506\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dxt\"].shape** =\n","        </td>\n","        <td>\n","           (3, 10)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"da_prev\"][2][3]** =\n","        </td>\n","        <td>\n","           -0.152399493774\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"da_prev\"].shape** =\n","        </td>\n","        <td>\n","           (5, 10)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dWax\"][3][1]** =\n","        </td>\n","        <td>\n","           0.410772824935\n","        </td>\n","    </tr>\n","            <tr>\n","        <td>\n","            **gradients[\"dWax\"].shape** =\n","        </td>\n","        <td>\n","           (5, 3)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dWaa\"][1][2]** = \n","        </td>\n","        <td>\n","           1.15034506685\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dWaa\"].shape** =\n","        </td>\n","        <td>\n","           (5, 5)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dba\"][4]** = \n","        </td>\n","        <td>\n","           [ 0.20023491]\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dba\"].shape** = \n","        </td>\n","        <td>\n","           (5, 1)\n","        </td>\n","    </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"-iKRDCSwELGg"},"source":["#### Passe backward du RNN\n","\n","Le calcul des gradients du coût par rapport à $a^{\\langle t \\rangle}$ à chaque pas de temps $t$ est utile car c'est ce qui aide le gradient à se rétropropager vers la cellule RNN précédente. Pour ce faire, vous devez itérer à travers tous les pas de temps en commençant à la fin, et à chaque étape, vous incrémentez $db_a$, $dW_{aa}$, $dW_{ax}$ et vous stockez $dx$.\n","\n","**Instructions** :\n","\n","Implémenter la fonction `rnn_backward`. Initialiser d'abord les variables de retour avec des zéros, puis faire une boucle à travers tous les pas de temps en appelant la fonction `rnn_cell_backward` à chaque pas de temps, mettre à jour les autres variables en conséquence."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-10-28T14:10:26.643586Z","start_time":"2020-10-28T14:10:26.630072Z"},"id":"G38ecvUDELGg"},"outputs":[],"source":["def rnn_backward(da, caches):\n","    \"\"\"\n","    Implémente la passe backward d'un RNN simple sur une séquence entière donnée en entrée.\n","\n","    Arguments :\n","    da -- Gradients amont de tous les états cachés, de shape (n_a, m, T_x)\n","    caches -- tuple contenant les variables de la passe forward (rnn_forward)\n","    \n","    Retourne :\n","    gradients -- dictionnaire python contenant :\n","                        dx -- Gradients par rapport aux input, de shape (n_x, m, T_x)\n","                        da0 -- Gradients par rapport à l'état caché initial, de shape (n_a, m)\n","                        dWax -- Gradients par rapport aux poids input-to-hidden, de shape (n_a, n_x)\n","                        dWaa -- Gradients par rapport aux poids hidden-to-hidden, de shape (n_a, n_a)\n","                        dba -- Gradients par rapport au vecteur biais, de shape (n_a, 1)\n","    \"\"\"\n","        \n","    ### A COMPLETER ###\n","    \n","    # Récupérer les variables du premier cache (t=1) de caches \n","    (caches, x) = None\n","    (a1, a0, x1, parameters) = None\n","    \n","    # Récupérer les dimensions de da et x1\n","    n_a, m, T_x = None\n","    n_x, m = None\n","    \n","    # Initialiser les gradients avec les bonnes dimensions\n","    dx = None\n","    dWax = None\n","    dWaa = None\n","    dba = None\n","    da0 = None\n","    da_prevt = None\n","    \n","    # Loop sur tous les pas de temps  \n","    for t in reversed(range(None)):\n","        # Calculer les gradients au temps t. \n","        # Sommer les gradients de la sortie (da) avec ceux des temps précédents (da_prevt) \n","        gradients = None\n","        # Récupérer les gradients \n","        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n","        # Incrémenter les dérivées par rapport aux paramètres en ajoutant leur valeur au temps t \n","        dx[:, :, t] = None\n","        dWax += None\n","        dWaa += None\n","        dba += None\n","        \n","    # Calculer da0, le gradient de a qui a été rétropropagé sur tous les pas de temps \n","    da0 = None\n","    ### FIN ###\n","\n","    # Stocker les gradients dans un dictionnaire\n","    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n","    \n","    return gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"oZXl_w5pELGj"},"outputs":[],"source":["np.random.seed(1)\n","x_tmp = np.random.randn(3,10,4)\n","a0_tmp = np.random.randn(5,10)\n","parameters_tmp = {}\n","parameters_tmp['Wax'] = np.random.randn(5,3)\n","parameters_tmp['Waa'] = np.random.randn(5,5)\n","parameters_tmp['Wya'] = np.random.randn(2,5)\n","parameters_tmp['ba'] = np.random.randn(5,1)\n","parameters_tmp['by'] = np.random.randn(2,1)\n","\n","a_tmp, y_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n","da_tmp = np.random.randn(5, 10, 4)\n","gradients_tmp = rnn_backward(da_tmp, caches_tmp)\n","\n","print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n","print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n","print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n","print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n","print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n","print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n","print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n","print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n","print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"]},{"cell_type":"markdown","metadata":{"id":"8JFS03nAELGl"},"source":["**Affichage attendu** :\n","\n","<table>\n","    <tr>\n","        <td>\n","            **gradients[\"dx\"][1][2]** =\n","        </td>\n","        <td>\n","           [-2.07101689 -0.59255627  0.02466855  0.01483317]\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dx\"].shape** =\n","        </td>\n","        <td>\n","           (3, 10, 4)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"da0\"][2][3]** =\n","        </td>\n","        <td>\n","           -0.314942375127\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"da0\"].shape** =\n","        </td>\n","        <td>\n","           (5, 10)\n","        </td>\n","    </tr>\n","         <tr>\n","        <td>\n","            **gradients[\"dWax\"][3][1]** =\n","        </td>\n","        <td>\n","           11.2641044965\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dWax\"].shape** =\n","        </td>\n","        <td>\n","           (5, 3)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dWaa\"][1][2]** = \n","        </td>\n","        <td>\n","           2.30333312658\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dWaa\"].shape** =\n","        </td>\n","        <td>\n","           (5, 5)\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dba\"][4]** = \n","        </td>\n","        <td>\n","           [-0.74747722]\n","        </td>\n","    </tr>\n","        <tr>\n","        <td>\n","            **gradients[\"dba\"].shape** = \n","        </td>\n","        <td>\n","           (5, 1)\n","        </td>\n","    </tr>\n","</table>"]}],"metadata":{"coursera":{"course_slug":"nlp-sequence-models","graded_item_id":"xxuVc","launcher_item_id":"X20PE"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":true,"toc_position":{},"toc_section_display":"block","toc_window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}