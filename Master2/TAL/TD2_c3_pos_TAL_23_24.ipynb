{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "id": "Gl0Si599hvkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import NLTK and a few packages\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('large_grammars')\n",
        "from IPython.display import display\n",
        "import svgling"
      ],
      "metadata": {
        "id": "D5RUBQNpvXkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Using NLTK tokenizer and pos tagger\n",
        "\n",
        "* nltk.word_tokenize(text) gives you a list of tokens\n",
        "* nltk.pos-tag(tokens) gives you a list of POS\n",
        "\n",
        "--> Extract the POS tags for the sentence \"Time flies like an arrow\", what do you obtain?"
      ],
      "metadata": {
        "id": "PDjuPMJOGG5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the tokens\n"
      ],
      "metadata": {
        "id": "3OLOSy-tvhpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the POS tags\n"
      ],
      "metadata": {
        "id": "BQXImqeAu-gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Lexico-syntactic patterns\n",
        "\n",
        "File used : discours_method.txt\n",
        "\n",
        "The code below allows to retrieve the POS tags for the document (we'll see this code in more details during the next practical session). For now:\n",
        "\n",
        "--> Retrieve the sequences of NOUN + ADJ\n",
        "\n",
        "--> print the 20 most frequent ones"
      ],
      "metadata": {
        "id": "nx_5VjWs2dIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myfile = \"discours_methode.txt\"\n",
        "\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "id": "Ek-NXYdC5bC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "# Read in string of characters\n",
        "with open( myfile  ) as infile:\n",
        "  text = infile.read()\n",
        "# Preprocess using spacy's pipeline\n",
        "doc = nlp(text)\n",
        "import numpy as np\n",
        "# Inspect token\n",
        "# Our preprocessed document is now present as a list of tokens in our doc variable, and we can access its different annotations by looping through it\n",
        "token_tag =  []\n",
        "for token in doc:\n",
        "  token_tag.append([token.text, token.pos_])\n",
        "print('Count tokens', len(token_tag))\n",
        "print('Count types', len(np.unique([t for (t,p) in token_tag])))\n",
        "\n",
        "# Print the first 30 tokens + POS\n",
        "for (t,p) in token_tag[:30]:\n",
        "  print(t,p)"
      ],
      "metadata": {
        "id": "sHwm7uttuhi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve sequences of NOUN + ADJ\n"
      ],
      "metadata": {
        "id": "Kizxx1ZU5pow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the 20 most frequent\n"
      ],
      "metadata": {
        "id": "0kilm1mvKR1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Find sequences including a VERB and a NOUN (e.g. Ã©tudier la nature)\n",
        "\n",
        "--> print the 20 most frequent ones"
      ],
      "metadata": {
        "id": "k4HGHEun8Cz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve sequences including a VERB and a NOUN\n"
      ],
      "metadata": {
        "id": "VXJdJoMD65MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the 20 most frequent\n",
        "counter = collections.Counter(verb_n)\n",
        "counter.most_common(20)"
      ],
      "metadata": {
        "id": "K7OodFiR75CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Retrieve proper names\n",
        "\n",
        "--> Retrieve the proper nouns / names using the POS tags\n",
        "\n",
        "--> Check the POS tagger accuracy: did the system make any mistakes?"
      ],
      "metadata": {
        "id": "lcclybdj3E89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQVqbBryuYHw"
      },
      "outputs": [],
      "source": [
        "# Retrieve proper nouns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check system accuracy\n"
      ],
      "metadata": {
        "id": "pNxV2XhrNO58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Using NLTK to obtain the parse trees\n",
        "\n",
        "Here we're going to use a symbolic parser, based on a hand-written grammar."
      ],
      "metadata": {
        "id": "6-g2Rv9Iz_Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With a simple grammar\n",
        "\n",
        "Look at the very simple grammar below: 'fish' can be either a Noun or a Verb in English.\n",
        "\n",
        "--> Parse the sentence 'fish fish fish' using the code in the second block. How many parses do you obtain?"
      ],
      "metadata": {
        "id": "zJkp2v-g0MiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "... S -> NP V NP\n",
        "... NP -> NP Sbar\n",
        "... Sbar -> NP V\n",
        "... NP -> 'fish'\n",
        "... V -> 'fish'\n",
        "... \"\"\")"
      ],
      "metadata": {
        "id": "xQXQpVs-FzZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [\"fish\"] * 5\n",
        "cp = nltk.ChartParser(grammar)\n",
        "for tree in cp.parse(tokens):\n",
        "  print(tree)"
      ],
      "metadata": {
        "id": "1tEL0F4kF4Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Now define a slightly more complex grammar that allows to get the 3 parses for the sentence 'Time flies like an arrow'"
      ],
      "metadata": {
        "id": "DqDMoNHCqJ2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "N -> 'flies' | 'banana'\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "L_b1TWQv05D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the parses\n",
        "tokens = nltk.word_tokenize('time flies like an arrow')\n"
      ],
      "metadata": {
        "id": "mzHWSgrB1Kt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generation\n",
        "\n",
        "A grammar can also be used to generate sentences.\n",
        "\n",
        "--> Take a look at the doc here: https://www.nltk.org/howto/generate.html and generate 42 possible sentences based on your grammar."
      ],
      "metadata": {
        "id": "9kzr2xZE8nx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 42 sentences based on your grammar"
      ],
      "metadata": {
        "id": "h3rjl1gm8n-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Write a small grammar allowing to generate sentences such as:\n",
        "- il fera chaud sur la France\n",
        "- le temps sera pluvieux\n",
        "\n",
        "--> Print 100 generated sentences\n",
        "\n",
        "--> How could we limit surgeneration?"
      ],
      "metadata": {
        "id": "xByfj3BD-d4G"
      }
    }
  ]
}